<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf8" />
    <meta name="viewport" content="initial-scale=1.0, width=device-width" />
    <title>
      
         | LMH.SITE
      
    </title>
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    
      <link rel="apple-touch-icon"
            sizes="180x180"
            href="/images/apple-touch-icon.png" />
    
    
      <link rel="icon"
            type="image/png"
            sizes="32x32"
            href="/images/favicon-32x32.png" />
    
    
      <link rel="icon"
            type="image/png"
            sizes="16x16"
            href="/images/favicon-16x16.png" />
    
    
      <link rel="mask-icon"
            href="/images/logo.svg"
            color="" />
    
    
    
      
  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/regular.ttf);
        font-weight: regular;
    }
  </style>

  <style>
    @font-face {
        font-family:sourceHanSerif;
        src: url(/font/bold.ttf);
        font-weight: bold;
    }
  </style>


    
    <link rel="stylesheet"
          type="text/css"
          href='/css/layout.css' />
    
    
  <link rel="stylesheet" type="text/css" href="/css/post.css" />
  

  <meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <div class="head">
      <div class="nav">
        <a href='/' class="nav-logo">
          <img alt="logo" height="60px" width="60px" src="/images/logo.svg" />
        </a>
        <input id="navBtn" type="checkbox" />
        <div class="nav-menu">
          
            
              <a class="nav-menu-item" href="/feat">学习记录</a>
            
              <a class="nav-menu-item" href="/life">生活</a>
            
          
        </div>
        <label class="nav-btn" for="navBtn"></label>
      </div>
    </div>
    <div class="body">
      
  <article class="post-content">
    <div class="post-inner">
      <div class="post-content__head">
        <div class="post-title">未命名</div>
        <div class="post-info">
          
  


          <span class="post-date">2023-07-18</span>
        </div>
      </div>
      
      <div class="post-content__body">
        
          <div class="post-gallery">
            
          </div>
        
        <p>category: Test title: test date: 2023-7-11 10:31:06 toc: true tags:
深度学习 mathjax: true timeline: article</p>
<h1 id="线性分类笔记">线性分类笔记</h1>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="">线性分类器简介</a></li>
<li><a href="">线性评分函数</a></li>
<li><a href="">阐明线性分类器</a></li>
<li><a href="">损失函数</a>
<ul>
<li><a href="">多类SVM</a></li>
<li><a href="">Softmax分类器</a></li>
<li><a href="">Softmax与SVM的比较</a></li>
</ul></li>
<li><a href="">小结</a></li>
</ul>
<h2 id="线性分类">线性分类</h2>
<p>上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest
Neighbor
（k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest
Neighbor分类器存在以下不足：</p>
<ul>
<li>分类器必须<em>记住</em>所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。</li>
<li>对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。</li>
</ul>
<p><strong>概述</strong>：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是<strong>评分函数（score
function）</strong>，它是原始图像数据到类别分值的映射。另一个是<strong>损失函数（loss
function）</strong>，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。</p>
<h2 id="从图像到标签分值的参数化映射">从图像到标签分值的参数化映射</h2>
<p>该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集<img
src="https://www.zhihu.com/equation?tex=x_i%5Cin+R%5ED"
alt="[公式]" />，每个图像都有一个对应的分类标签<img
src="https://www.zhihu.com/equation?tex=y_i" alt="[公式]" />。这里<img
src="https://www.zhihu.com/equation?tex=i%3D1%2C2...N"
alt="[公式]" />并且<img
src="https://www.zhihu.com/equation?tex=y_i%5Cin+1...K"
alt="[公式]" />。这就是说，我们有<strong>N</strong>个图像样例，每个图像的维度是<strong>D</strong>，共有<strong>K</strong>种不同的分类。</p>
<p>举例来说，在CIFAR-10中，我们有一个<strong>N</strong>=50000的训练集，每个图像有<strong>D</strong>=32x32x3=3072个像素，而<strong>K</strong>=10，这是因为图片被分为10个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：<img
src="https://www.zhihu.com/equation?tex=f%3AR%5ED%5Cto+R%5EK"
alt="[公式]" />，该函数是原始图像像素到分类分值的映射。</p>
<p><strong>线性分类器</strong>：在本模型中，我们从最简单的概率函数开始，一个线性映射：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>在上面的公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D
x 1]。其中大小为[K x D]的矩阵<strong>W</strong>和大小为[K x
1]列向量<strong>b</strong>为该函数的<strong>参数（parameters）</strong>。还是以CIFAR-10为例，<img
src="https://www.zhihu.com/equation?tex=x_i"
alt="[公式]" />就包含了第i个图像的所有像素信息，这些信息被拉成为一个[3072
x
1]的列向量，<strong>W</strong>大小为[10x3072]，<strong>b</strong>的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数<strong>W</strong>被称为<strong>权重（weights）</strong>。<strong>b</strong>被称为<strong>偏差向量（bias
vector）</strong>，这是因为它影响输出数值，但是并不和原始数据<img
src="https://www.zhihu.com/equation?tex=x_i"
alt="[公式]" />产生关联。在实际情况中，人们常常混用<strong>权重</strong>和<strong>参数</strong>这两个术语。</p>
<p>需要注意的几点：</p>
<ul>
<li>首先，一个单独的矩阵乘法<img
src="https://www.zhihu.com/equation?tex=Wx_i"
alt="[公式]" />就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>注意我们认为输入数据<img
src="https://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29"
alt="[公式]" />是给定且不可改变的，但参数<strong>W</strong>和<strong>b</strong>是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。</li>
<li>该方法的一个优势是训练数据是用来学习到参数<strong>W</strong>和<strong>b</strong>的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。</li>
<li>最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。</li>
</ul>
<blockquote>
<p><em>预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射<strong>(f)</strong>就要复杂多了，其包含的参数也更多。</em></p>
</blockquote>
<h2 id="理解线性分类器">理解线性分类器</h2>
<p>线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。</p>
<p><img src="images/15.jpg" /></p>
<p>一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。</p>
<p>—————————————————————————————————————————————————</p>
<p><strong>将图像看做高维度的点</strong>：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。</p>
<p>既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：</p>
<p>—————————————————————————————————————————————————</p>
<p><img src="images/16.jpg" /></p>
<p>图像空间的示意图。其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。</p>
<p>—————————————————————————————————————————————————</p>
<p>从上面可以看到，<strong>W</strong>的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差<strong>b</strong>，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在<img
src="https://www.zhihu.com/equation?tex=x_i%3D0"
alt="[公式]" />时分类分值始终为0。这样所有分类器的线都不得不穿过原点。</p>
<p><strong>将线性分类器看做模板匹配</strong>：关于权重<strong>W</strong>的另一个解释是<strong>它</strong>的每一行对应着一个分类的模板（有时候也叫作<em>原型</em>）。一张图像对应不同分类的得分，是通过使用内积（也叫<em>点积</em>）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。</p>
<p>—————————————————————————————————————————————————</p>
<p><img src="images/17.jpg" /></p>
<p>将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。</p>
<p>—————————————————————————————————————————————————</p>
<p>可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。</p>
<p><strong>偏差和权重的合并技巧</strong>：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数<img
src="https://www.zhihu.com/equation?tex=W" alt="[公式]" />和<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />合二为一。回忆一下，分类评分函数定义为：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%2Cb%29%3DWx_i%2Bb"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>分开处理这两个参数（权重参数<img
src="https://www.zhihu.com/equation?tex=W" alt="[公式]" />和偏差参数<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时<img
src="https://www.zhihu.com/equation?tex=x_i"
alt="[公式]" />向量就要增加一个维度，这个维度的数值是常量1，这就是默认的<em>偏差维度</em>。这样新的公式就简化成下面这样：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x_i%2CW%29%3DWx_i"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>还是以CIFAR-10为例，那么<img
src="https://www.zhihu.com/equation?tex=x_i"
alt="[公式]" />的大小就变成<strong>[3073x1]</strong>，而不是[3072x1]了，多出了包含常量1的1个维度）。W大小就是<strong>[10x3073]</strong>了。<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />中多出来的这一列对应的就是偏差值<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />，具体见下图：</p>
<p>—————————————————————————————————————————————————</p>
<p><img src="images/18.jpg" /></p>
<p>偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。</p>
<p>—————————————————————————————————————————————————</p>
<p><strong>图像数据预处理</strong>：在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来<strong>中心化</strong>数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127,
127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1,
1]。<strong>零均值的中心化</strong>是很重要的，等我们理解了梯度下降后再来详细解释。</p>
<h2 id="损失函数-loss-function">损失函数 Loss function</h2>
<p>在上一节定义了从图像像素值到所属类别的评分函数（score
function），该函数的参数是权重矩阵<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />。在函数中，数据<img
src="https://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29"
alt="[公式]" />是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p>
<p>回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<strong>损失函数（Loss
Function）</strong>（有时也叫<strong>代价函数Cost
Function</strong>或<strong>目标函数Objective</strong>）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p>
<h2
id="多类支持向量机损失-multiclass-support-vector-machine-loss">多类支持向量机损失
Multiclass Support Vector Machine Loss</h2>
<p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值<img
src="https://www.zhihu.com/equation?tex=%5CDelta"
alt="[公式]" />。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p>
<p>让我们更精确一些。回忆一下，第i个数据中包含图像<img
src="https://www.zhihu.com/equation?tex=x_i"
alt="[公式]" />的像素和代表正确类别的标签<img
src="https://www.zhihu.com/equation?tex=y_i"
alt="[公式]" />。评分函数输入像素数据，然后通过公式<img
src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29"
alt="[公式]" />来计算不同分类类别的分值。这里我们将分值简写为<img
src="https://www.zhihu.com/equation?tex=s"
alt="[公式]" />。比如，针对第j个类别的得分就是第j个元素：<img
src="https://www.zhihu.com/equation?tex=s_j%3Df%28x_i%2CW%29_j"
alt="[公式]" />。针对第i个数据的多类SVM的损失函数定义如下： <span
class="math display">\[L_i=\Sigma_{j\neq
y_i}max(0,s_j-s_{y_i}+\Delta)\]</span></p>
<p><strong>举例</strong>：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值<img
src="https://www.zhihu.com/equation?tex=s%3D%5B13%2C-7%2C11%5D"
alt="[公式]" />。其中第一个类别是正确类别，即<img
src="https://www.zhihu.com/equation?tex=y_i%3D0"
alt="[公式]" />。同时假设<img
src="https://www.zhihu.com/equation?tex=%5CDelta"
alt="[公式]" />是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（<img
src="https://www.zhihu.com/equation?tex=j%5Cnot%3Dy_i"
alt="[公式]" />）加起来，所以我们得到两个部分：</p>
<p><span
class="math display">\[L_i=max(0,-7-13+10)+max(0,11-13+10)\]</span></p>
<p>可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过<img
src="https://www.zhihu.com/equation?tex=max%280%2C-%29"
alt="[公式]" />函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别<img
src="https://www.zhihu.com/equation?tex=y_i"
alt="[公式]" />的分数比不正确类别分数高，而且至少要高<img
src="https://www.zhihu.com/equation?tex=%5CDelta"
alt="[公式]" />。如果不满足这点，就开始计算损失值。</p>
<p>那么在这次的模型中，我们面对的是线性评分函数（<img
src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29%3DWx_i"
alt="[公式]" />），所以我们可以将损失函数的公式稍微改写一下：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cw%5ET_jx_i-w%5ET_%7By_i%7Dx_i%2B%5CDelta%29"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中<img src="https://www.zhihu.com/equation?tex=w_j"
alt="[公式]" />是权重<img src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />的第j行，被变形为行向量。然而，一旦开始考虑更复杂的评分函数<img
src="https://www.zhihu.com/equation?tex=f"
alt="[公式]" />公式，这样做就不是必须的了。</p>
<p>在结束这一小节前，还必须提一下的属于是关于0的阀值：<img
src="https://www.zhihu.com/equation?tex=max%280%2C-%29"
alt="[公式]" />函数，它常被称为<strong>折叶损失（hinge
loss）</strong>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是<img
src="https://www.zhihu.com/equation?tex=max%280%2C-%29%5E2"
alt="[公式]" />，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p>
<blockquote>
<p>我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。</p>
</blockquote>
<p>—————————————————————————————————————————————————</p>
<p><img src="images/19.jpg" /></p>
<p>多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</p>
<p>—————————————————————————————————————————————————</p>
<p>对于下图中数据样本点的拟合，可以用一次函数，也可以用二次多项式函数，可以发现效果更好，但也可以利用五次多项式，此时就可以完全拟合。然而，第三种在训练集中可以很好拟合，并不能在之外的数据呈现好的效果，这就是没有好的泛化能力，我们可以用<strong>泛化误差</strong>来衡量。
第一种建立了一个线性模型，但是该模型并没有精准地捕捉到训练集数据结构，此时称为有较大的<strong>偏倚</strong>，也称欠拟合，第三种通过5次多项式函数很好的对样本进行了拟合，然而，如果将建立的模型进行泛化，并不能很好的对训练集之外数据进行预测，此时称为有较大<strong>方差</strong>，也称过拟合。</p>
<p><img src="images/32.png" /></p>
<p>借鉴机器学习中相关知识，在不同的拟合模型中有VC理论：假设有n歌样本点<span
class="math inline">\((x_1,y_1),...,(x_n,y_n),x_i \in R^d,y_i \in
{0,1}\)</span>，对于一个分类器f，如果f可辩别，则 <span
class="math display">\[ P(\exists f \in F, P_{(x,y)~D}(y \neq
f(x))-\frac{1}{n} \sum_{i=1}^{n}I(y_i \neq f(x_i)) \geq \epsilon) \leq
N^\Phi (2n)2e^{-\frac{1}{2}n\epsilon^2}\]</span></p>
<p>通常，在偏倚和方差之间，这样一种规律：如果模型过于简单，其具有大的偏倚，而如果模型过于复杂，它就有大的方差。调整模型的复杂度，建立适当的误差模型，就变得极其重要了。所以我们引入正则化项</p>
<p><strong>正则化（Regularization）：</strong>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有<img
src="https://www.zhihu.com/equation?tex=L_i%3D0"
alt="[公式]" />）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当<img
src="https://www.zhihu.com/equation?tex=%5Clambda%3E1"
alt="[公式]" />时，任何数乘<img
src="https://www.zhihu.com/equation?tex=%5Clambda+W"
alt="[公式]" />都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p>换句话说，我们希望能向某些特定的权重<strong>W</strong>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<strong>正则化惩罚（regularization
penalty）</strong><img src="https://www.zhihu.com/equation?tex=R%28W%29"
alt="[公式]" />部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=R%28W%29%3D%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>上面的表达式中，将<img src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong>数据损失（data
loss）</strong>，即所有样例的的平均损失<img
src="https://www.zhihu.com/equation?tex=L_i"
alt="[公式]" />，以及<strong>正则化损失（regularization
loss）</strong>。完整公式如下所示：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=L%3D%5Cdisplaystyle+%5Cunderbrace%7B+%5Cfrac%7B1%7D%7BN%7D%5Csum_i+L_i%7D_%7Bdata+%5C++loss%7D%2B%5Cunderbrace%7B%5Clambda+R%28W%29%7D_%7Bregularization+%5C+loss%7D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>将其展开完整公式是：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_i%5Csum_%7Bj%5Cnot%3Dy_i%7D%5Bmax%280%2Cf%28x_i%3BW%29_j-f%28x_i%3BW%29_%7By_i%7D%2B%5CDelta%29%5D%2B%5Clambda+%5Csum_k+%5Csum_l+W%5E2_%7Bk%2Cl%7D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中，<img src="https://www.zhihu.com/equation?tex=N"
alt="[公式]" />是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数<img
src="https://www.zhihu.com/equation?tex=%5Clambda"
alt="[公式]" />来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p>
<p>除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了<strong>最大边界（max
margin）</strong>这一良好性质。</p>
<p>其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量<img
src="https://www.zhihu.com/equation?tex=x%3D%5B1%2C1%2C1%2C1%5D"
alt="[公式]" />，两个权重向量<img
src="https://www.zhihu.com/equation?tex=w_1%3D%5B1%2C0%2C0%2C0%5D"
alt="[公式]" />，<img
src="https://www.zhihu.com/equation?tex=w_2%3D%5B0.25%2C0.25%2C0.25%2C0.25%5D"
alt="[公式]" />。那么<img
src="https://www.zhihu.com/equation?tex=w%5ET_1x%3Dw%5ET_2%3D1"
alt="[公式]" />，两个权重向量都得到同样的内积，但是<img
src="https://www.zhihu.com/equation?tex=w_1"
alt="[公式]" />的L2惩罚是1.0，而<img
src="https://www.zhihu.com/equation?tex=w_2"
alt="[公式]" />的L2惩罚是0.25。因此，根据L2惩罚来看，<img
src="https://www.zhihu.com/equation?tex=w_2"
alt="[公式]" />更好，因为它的正则化损失更小。从直观上来看，这是因为<img
src="https://www.zhihu.com/equation?tex=w_2"
alt="[公式]" />的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<em>过拟合</em>。</p>
<p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />正则化，而不正则化偏差<img
src="https://www.zhihu.com/equation?tex=b"
alt="[公式]" />。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当<img
src="https://www.zhihu.com/equation?tex=W%3D0"
alt="[公式]" />的特殊情况下，才能得到损失值为0。</p>
<p><strong>代码</strong>：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">L_i</span>(<span class="hljs-params">x, y, W</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span><br><span class="hljs-string">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span><br><span class="hljs-string">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span><br><span class="hljs-string">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span><br><span class="hljs-string">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  delta = <span class="hljs-number">1.0</span> <span class="hljs-comment"># see notes about delta later in this section</span><br>  scores = W.dot(x) <span class="hljs-comment"># scores becomes of size 10 x 1, the scores for each class</span><br>  correct_class_score = scores[y]<br>  D = W.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># number of classes, e.g. 10</span><br>  loss_i = <span class="hljs-number">0.0</span><br>  <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(D): <span class="hljs-comment"># iterate over all wrong classes</span><br>    <span class="hljs-keyword">if</span> j == y:<br>      <span class="hljs-comment"># skip for the true class to only loop over incorrect classes</span><br>      <span class="hljs-keyword">continue</span><br>    <span class="hljs-comment"># accumulate loss for the i-th example</span><br>    loss_i += <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, scores[j] - correct_class_score + delta)<br>  <span class="hljs-keyword">return</span> loss_i<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">L_i_vectorized</span>(<span class="hljs-params">x, y, W</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  A faster half-vectorized implementation. half-vectorized</span><br><span class="hljs-string">  refers to the fact that for a single example the implementation contains</span><br><span class="hljs-string">  no for loops, but there is still one loop over the examples (outside this function)</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  delta = <span class="hljs-number">1.0</span><br>  scores = W.dot(x)<br>  <span class="hljs-comment"># compute the margins for all classes in one vector operation</span><br>  margins = np.maximum(<span class="hljs-number">0</span>, scores - scores[y] + delta)<br>  <span class="hljs-comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span><br>  <span class="hljs-comment"># to ignore the y-th position and only consider margin on max wrong class</span><br>  margins[y] = <span class="hljs-number">0</span><br>  loss_i = np.<span class="hljs-built_in">sum</span>(margins)<br>  <span class="hljs-keyword">return</span> loss_i<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">L</span>(<span class="hljs-params">X, y, W</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  fully-vectorized implementation :</span><br><span class="hljs-string">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span><br><span class="hljs-string">  - y is array of integers specifying correct class (e.g. 50,000-D array)</span><br><span class="hljs-string">  - W are weights (e.g. 10 x 3073)</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-comment"># evaluate loss over all examples in X without using any for loops</span><br>  <span class="hljs-comment"># left as exercise to reader in the assignment</span><br></code></pre></td></tr></table></figure>
<p>在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。</p>
<blockquote>
<p>接下来要做的，就是找到能够使损失值最小化的权重了。</p>
</blockquote>
<h2 id="实际考虑">实际考虑</h2>
<p><strong>设置Delta</strong>：你可能注意到上面的内容对超参数<img
src="https://www.zhihu.com/equation?tex=%5CDelta"
alt="[公式]" />及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为<img
src="https://www.zhihu.com/equation?tex=%5CDelta%3D1.0"
alt="[公式]" />都是安全的。超参数<img
src="https://www.zhihu.com/equation?tex=%5CDelta" alt="[公式]" />和<img
src="https://www.zhihu.com/equation?tex=%5Clambda"
alt="[公式]" />看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如<img
src="https://www.zhihu.com/equation?tex=%5CDelta%3D1"
alt="[公式]" />或<img
src="https://www.zhihu.com/equation?tex=%5CDelta%3D100"
alt="[公式]" />）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度<img
src="https://www.zhihu.com/equation?tex=%5Clambda"
alt="[公式]" />来控制）。</p>
<p><strong>与二元支持向量机（Binary Support Vector
Machine）的关系</strong>：在学习本课程前，我们可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3DCmax%280%2C1-y_iw%5ETx_i%29%2BR%28W%29"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中，<img src="https://www.zhihu.com/equation?tex=C"
alt="[公式]" />是一个超参数，并且<img
src="https://www.zhihu.com/equation?tex=y_i%5Cin%5C%7B-1%2C1%5C%7D"
alt="[公式]" />。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的<img
src="https://www.zhihu.com/equation?tex=C"
alt="[公式]" />和多类SVM公式中的<img
src="https://www.zhihu.com/equation?tex=%5Clambda"
alt="[公式]" />都控制着同样的权衡，而且它们之间的关系是<img
src="https://www.zhihu.com/equation?tex=C%5Cpropto%5Cfrac%7B1%7D%7B%5Clambda%7D"
alt="[公式]" /></p>
<h2 id="softmax分类器">Softmax分类器</h2>
<p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出<img
src="https://www.zhihu.com/equation?tex=f%28x_i%2CW%29"
alt="[公式]" />作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射<img
src="https://www.zhihu.com/equation?tex=f%28x_i%3BW%29%3DWx_i"
alt="[公式]" />保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge
loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy
loss）</strong>。公式如下：</p>
<p><img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Li%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29"
alt="[公式]" /> 或等价的 <img
src="https://www.zhihu.com/equation?tex=L_i%3D-f_%7By_i%7D%2Blog%28%5Csum_je%5E%7Bf_j%7D%29"
alt="[公式]" /></p>
<p>在上式中，使用<img src="https://www.zhihu.com/equation?tex=f_j"
alt="[公式]" />来表示分类评分向量<img
src="https://www.zhihu.com/equation?tex=f"
alt="[公式]" />中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值<img
src="https://www.zhihu.com/equation?tex=L_i"
alt="[公式]" />的均值与正则化损失<img
src="https://www.zhihu.com/equation?tex=R%28W%29"
alt="[公式]" />之和。其中函数<img
src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D"
alt="[公式]" />被称作<strong>softmax
函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（<img
src="https://www.zhihu.com/equation?tex=z"
alt="[公式]" />中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布<img
src="https://www.zhihu.com/equation?tex=p" alt="[公式]" />和估计分布<img
src="https://www.zhihu.com/equation?tex=q"
alt="[公式]" />之间的<em>交叉熵</em>定义如下：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>因此，Softmax分类器所做的就是最小化在估计分类概率（就是上面的<img
src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D%2F%5Csum_je%5E%7Bf_j%7D"
alt="[公式]" />）和“真实”分布之间的交叉熵，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：<img
src="https://www.zhihu.com/equation?tex=p%3D%5B0%2C...1%2C...%2C0%5D"
alt="[公式]" />中在<img src="https://www.zhihu.com/equation?tex=y_i"
alt="[公式]" />的位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵（Kullback-Leibler
divergence）<img
src="https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3DH%28p%29%2BD_%7BKL%7D%28p%7C%7Cq%29"
alt="[公式]" />，并且delta函数<img
src="https://www.zhihu.com/equation?tex=p"
alt="[公式]" />的熵是0，那么就能等价的看做是对两个分布之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。</p>
<p><strong>注</strong>：Kullback-Leibler差异（Kullback-Leibler
Divergence）也叫做相对熵（Relative
Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。</p>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=P%28y_i%7Cx_i%2CW%29%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>可以解释为是给定图像数据<img
src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" />，以<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />为参数，分配给正确分类标签<img
src="https://www.zhihu.com/equation?tex=y_i"
alt="[公式]" />的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量<img
src="https://www.zhihu.com/equation?tex=f"
alt="[公式]" />中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行<em>最大似然估计</em>（MLE）。该解释的另一个好处是，损失函数中的正则化部分<img
src="https://www.zhihu.com/equation?tex=R%28W%29"
alt="[公式]" />可以被看做是权重矩阵<img
src="https://www.zhihu.com/equation?tex=W"
alt="[公式]" />的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。</p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项<img
src="https://www.zhihu.com/equation?tex=e%5E%7Bf_%7By_i%7D%7D"
alt="[公式]" />和<img
src="https://www.zhihu.com/equation?tex=%5Csum_j+e%5E%7Bf_j%7D"
alt="[公式]" />因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数<img
src="https://www.zhihu.com/equation?tex=C"
alt="[公式]" />，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7BCe%5E%7Bf_%7By_i%7D%7D%7D%7BC%5Csum_je%5E%7Bf_j%7D%7D%3D%5Cfrac%7Be%5E%7Bf_%7By_i%7D%2BlogC%7D%7D%7B%5Csum_je%5E%7Bf_j%2BlogC%7D%7D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p><img src="https://www.zhihu.com/equation?tex=C"
alt="[公式]" />的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将<img
src="https://www.zhihu.com/equation?tex=C" alt="[公式]" />设为<img
src="https://www.zhihu.com/equation?tex=logC%3D-max_jf_j"
alt="[公式]" />。该技巧简单地说，就是应该将向量<img
src="https://www.zhihu.com/equation?tex=f"
alt="[公式]" />中的数值进行平移，使得最大值为0。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">f = np.array([<span class="hljs-number">123</span>, <span class="hljs-number">456</span>, <span class="hljs-number">789</span>]) <span class="hljs-comment"># 例子中有3个分类，每个评分的数值都很大</span><br>p = np.exp(f) / np.<span class="hljs-built_in">sum</span>(np.exp(f)) <span class="hljs-comment"># 不妙：数值问题，可能导致数值爆炸</span><br><br><span class="hljs-comment"># 那么将f中的值平移到最大值为0：</span><br>f -= np.<span class="hljs-built_in">max</span>(f) <span class="hljs-comment"># f becomes [-666, -333, 0]</span><br>p = np.exp(f) / np.<span class="hljs-built_in">sum</span>(np.exp(f)) <span class="hljs-comment"># 现在OK了，将给出正确结果</span><br></code></pre></td></tr></table></figure>
<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge
loss）</em>，有时候又被称为<em>最大边界损失（max-margin
loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy
loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax
loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="svm和softmax的比较">SVM和Softmax的比较</h2>
<p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<p>—————————————————————————————————————————————————</p>
<p><img src="images/20.png" /></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>—————————————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5,
0.6,
-23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9,
0.09,
0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1,
-2, 0]，那么softmax函数就会计算：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5B1%2C-2%2C0%5D%5Cto%5Be%5E1%2Ce%5E%7B-2%7D%2Ce%5E0%5D%3D%5B2.71%2C0.14%2C1%5D%5Cto%5B0.7%2C0.04%2C0.26%5D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5,
-1, 0]，那么softmax函数的计算就是：</p>
<figure>
<img
src="https://www.zhihu.com/equation?tex=%5B0.5%2C-1%2C0%5D%5Cto%5Be%5E%7B0.5%7D%2Ce%5E%7B-1%7D%2Ce%5E0%5D%3D%5B1.65%2C0.73%2C1%5D%5Cto%5B0.55%2C0.12%2C0.33%5D"
alt="[公式]" />
<figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local
objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10,
-2, 3]的数据，其中第一个分类是正确的。那么一个SVM（<img
src="https://www.zhihu.com/equation?tex=%5CDelta+%3D1"
alt="[公式]" />）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10,
-100, -100]或者[10, 9,
9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9,
9]来说，计算出的损失值就远远高于[10, -100,
-100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>
<h2 id="小结">小结</h2>
<p>总结如下：</p>
<ul>
<li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li>
<li>与kNN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li>
<li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li>
<li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li>
</ul>

      </div>
    </div>
  </article>
  <div class="post__foot">
    
    <div class="post-nav">
  
    <a class="post-nav-item-left" href="/2023/08/26/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%85%A5%E5%AD%A6/">
      <div class="text-align">
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
        <span class="text-small">上一篇</span>
      </div>
      <div>研究生入学</div>
    </a>
  
  <div class="vhr"></div>
  
    <a class="post-nav-item-right" href="/2023/07/11/CS231n2/">
      <div class="text-align">
        <span class="text-small">下一篇</span>
        <svg t="1670570876164"
             class="icon"
             viewBox="0 0 1024 1024"
             transform="scale(-1,-1)"
             width="16"
             height="16">
          <path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="14596"/>
        </svg>
      </div>
      CS231n：2 线性分类器
    </a>
  
</div>

    
    
  </div>

    </div>
    <div class="foot">
      <div class="foot-inner">
        <div class="foot__head">
          
            <div class="foot-line">
              <div class="matts">海</div><div class="matts">内</div><div class="matts">存</div><div class="matts">知</div><div class="matts">己</div>
            </div>
          
            <div class="foot-line">
              <div class="matts">天</div><div class="matts">涯</div><div class="matts">若</div><div class="matts">比</div><div class="matts">邻</div>
            </div>
          
        </div>
        <div class="foot__body">
          
            <div class="foot-item">
              <div class="foot-item__head">朋友</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/icon/icon-link.svg" />
                    <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-tranquility">Theme Tranquility</a>
                  </div>
                
                <div class="text">
                  <img alt="link" height="20px" width="20px" src="/images/icon/icon-link+.svg" />
                  <a class="foot-link"
                     href="mailto:1172456088@qq.com?subject=%E7%94%B3%E8%AF%B7%20Hozen.site%20%E7%9A%84%E5%8F%8B%E9%93%BE%E4%BD%8D%E7%BD%AE">
                  申请友链</a>
                </div>
              </div>
            </div>
          
          
            <div class="foot-item">
              <div class="foot-item__head">账号</div>
              <div class="foot-item__body">
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/logo-github.svg" />
                    <a class="foot-link" target="_blank" rel="noopener" href="https://github.com/">github</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/logo-wx.svg" />
                    <a class="foot-link" href="">微信公众号</a>
                  </div>
                
                  <div class="text">
                    <img alt="link" height="20px" width="20px" src="/images/logo-zh.svg" />
                    <a class="foot-link" target="_blank" rel="noopener" href="https://www.zhihu.com/">知乎</a>
                  </div>
                
              </div>
            </div>
          
          <div class="foot-item">
            <div class="foot-item__head">联系</div>
            <div class="foot-item__body">
              <div class="text">
                <img alt="link" height="20px" width="20px" src="/images/icon/icon-email.svg" />
                <a class="foot-link" href="mailto:1172456088@qq.com">1172456088@qq.com</a>
              </div>
            </div>
          </div>
        </div>
        <div class="copyright">
          <a href="http://example.com">LMH.SITE</a> &nbsp;|&nbsp;由&nbsp;<a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>&nbsp;及&nbsp;
          <svg width="20" height="20" viewBox="0 0 725 725">
            <path fill-rule="evenodd" fill="rgb(221, 221, 221)" d="M145.870,236.632 L396.955,103.578 L431.292,419.44 L156.600,522.53 L145.870,236.632 Z" />
            <path fill-rule="evenodd" fill="rgb(159, 159, 159)" d="M396.955,103.578 L564.345,234.486 L611.558,513.469 L431.292,419.44 L396.955,103.578 Z" />
            <path fill-rule="evenodd" fill="rgb(0, 0, 0)" d="M431.292,419.44 L611.558,513.469 L358.327,595.18 L156.600,522.53 L431.292,419.44 Z" />
          </svg>
          <a target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-tranquility">致远</a>&nbsp;驱动
        </div>
      </div>
    </div>
    
    
  

  </body>
</html>
